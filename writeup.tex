\documentclass[12pt]{article}
\usepackage{booktabs,ctable,multirow}
\usepackage{float}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{soul}
\usepackage{listings}

\lstset{breaklines=true}

\DeclareMathOperator{\C}{\mathbb{C}}
\DeclareMathOperator{\I}{\mathbb{I}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\Z}{\mathbb{Z}}

\DeclareRobustCommand{\E}[1]{\mathbb{E}\left[#1\right]}
\DeclareRobustCommand{\prob}[1]{\mathbb{P}\left(#1\right)}

\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\bias}{bias}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\diag}{diag}

\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\textheight 240mm
\textwidth  170mm
\oddsidemargin  0mm
\evensidemargin 0mm
\topmargin -20mm

\begin{document}

\begin{flushright}
    Dhamma Kimpara and Sam Zhang

    ECEN 5008-005 -- Homework \#4
\end{flushright}

\section*{Problem 1}%
\label{sec:Problem 1}

\begin{enumerate}
    \item [i)] Yes, the cost is strongly convex. Write $f_t(x) = \frac{1}{2}\norm{Ax - b_t}_2^2$, and $h_t(x) = Ax - b_t$. Then $f_t$ is $\mu$-strongly convex if the smallest eigenvalue of its Hessian $\nabla^2 f_t(x) = A$ is positive, with $\mu$ equal to that eigenvalue. By construction, then, $\mu = 1/\sqrt{\kappa}$.

    \item[ii)] We know from class that for convergence, we require the step size $\alpha$ to be in the range $(0, 2/L]$. Since $L$ is the Lipschitz constant of $\nabla f = Ax - b_t$, we have that since

        \begin{equation*}
            \norm{Ax - b_t - (Ay - b_t)} = \norm{Ax - Ay} \le \norm{A} \norm{x - y},
        \end{equation*}

        that $L \le \norm{A} = 1$, since the largest singular value of $A$ is $1$. Thus we must pick $\alpha \in (0, 2]$. The bound we derived in class uses 

        \begin{equation*}
            \rho = \max \lbrace \abs{ 1 - \alpha \mu}, \abs{ 1 - \alpha L } \rbrace = \max \lbrace \abs{ 1 - \alpha/\sqrt{\kappa}}, \abs{ 1 - \alpha } \rbrace.
        \end{equation*}

        This gives the finite bound

        \begin{equation}
            \label{eq:bounds}
            \norm{x_t - x_t^*} \le \rho^t \norm{x_0 - x_0^*} + \sigma \sum_{i=0}^t \rho^i = \rho^t \norm{x_0 - x_0^*} + \sigma \frac{1 - \rho^{t}}{1-\rho}
        \end{equation}

        with the asymptotic result

        \begin{equation*}
            \limsup_{t \to \infty} \norm{x_t - x_t^*} \le \frac{\sigma}{1 - \rho}.
        \end{equation*}

        With this specific problem, for $\sigma = 1$, since $\kappa = 100$, we have that for an arbitrary pick of $\alpha = 1$ that $\rho=91/100$, so the asymptotic bound is $1/(1-\rho) = 10$. As seen in Figure \ref{fig:p1ii}, the tracking error does not appear to typically approach either the finite or asymptotic bounds here.

        \begin{figure}[h]
        \begin{center}
            \includegraphics[scale=0.5]{figures/hw4-p1-ii.png}
        \end{center}
        \caption{Tracking error of one run of online gradient descent vs. finite bounds}
        \label{fig:p1ii}
        \end{figure}
    \item [iii)] In Figure \ref{fig:p1iii} we display a run using $\sigma=0.1$ vs. its corresponding bound, as well as a run using $\sigma=2.0$ using its bound. Since the optimal value moves around more when $\sigma$ is larger, we are unsurprised to see both the bound and the tracking error lowered for smaller $\sigma$.

    	\begin{figure}[h]
    		\begin{center}
    			\includegraphics[scale=0.5]{figures/hw4-p1-iii.png}
    		\end{center}
            \caption{Tracking error of one run of online gradient descent vs. finite bounds for $\sigma = 0.1$ and $\sigma = 2.0$. We display the asymptotic error for $\sigma=0.1$ since it is visible on the figure. For $\sigma=2$, the bound converges to $20$, which skews the axis too much.}
    		\label{fig:p1iii}
    	\end{figure}
    

\end{enumerate}

\section*{Problem 2}%
\label{sec:Problem 2}

We claimed in class that the bounds in Equation \ref{eq:bounds} holds for projected gradient descent as well. We find that the performance of PGD falls well within these bounds, perhaps simply because the maximum distance between any two points on the hypersphere is $2$, which we drew in Figure \ref{fig:p2} as a solid dashed line. To actually evaluate the effectiveness of the algorithm then, we compared the tracking bounds to the expected distance between two
uniformly randomly selected points on the 5-dimensional hypersphere (referring to this formula on StackExchange: https://math.stackexchange.com/q/2366593). The tracking error is indeed consistently below this theoretical baseline, confirming that our algorithm is working better than chance.

\begin{figure}[h]
\begin{center}
    \includegraphics[scale=0.5]{figures/hw4-p2.png}
\end{center}
\caption{Tracking error of one run of projected online gradient descent vs. finite bounds and some theoretical quantities related to points on a 5-dimensional hypersphere.}
\label{fig:p2}
\end{figure}

\section*{Code}
\label{sec:Code}

\subsection*{generator.py}
\lstinputlisting[language=Python]{generator.py}
\subsection*{optimizer.py}
\lstinputlisting[language=Python]{optimizer.py}

\end{document}
